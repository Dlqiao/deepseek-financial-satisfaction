"""
æ»¡æ„åº¦é¢„æµ‹æ¨¡å‹ï¼šç”¨æˆ·æ»¡æ„åº¦é¢„æµ‹ã€ç‰¹å¾é‡è¦æ€§åˆ†æã€æ¨¡å‹è§£é‡Š
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                            f1_score, roc_auc_score, confusion_matrix,
                            classification_report, roc_curve)
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import lightgbm as lgb
import joblib
import logging
from typing import Dict, List, Tuple, Optional, Any
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SatisfactionPredictor:
    """
    ç”¨æˆ·æ»¡æ„åº¦é¢„æµ‹æ¨¡å‹
    """
    
    def __init__(self, model_type: str = 'xgboost', random_state: int = 42):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('xgboost', 'lightgbm', 'random_forest', 'logistic')
            random_state: éšæœºç§å­
        """
        self.model_type = model_type
        self.random_state = random_state
        self.model = None
        self.scaler = StandardScaler()
        self.feature_importance = None
        self.metrics = {}
        self.feature_names = None
        
    def _init_model(self, **params):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.model_type == 'xgboost':
            default_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': self.random_state,
                'use_label_encoder': False,
                'eval_metric': 'logloss'
            }
            default_params.update(params)
            self.model = xgb.XGBClassifier(**default_params)
            
        elif self.model_type == 'lightgbm':
            default_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': self.random_state,
                'verbosity': -1
            }
            default_params.update(params)
            self.model = lgb.LGBMClassifier(**default_params)
            
        elif self.model_type == 'random_forest':
            default_params = {
                'n_estimators': 100,
                'max_depth': 10,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'random_state': self.random_state
            }
            default_params.update(params)
            self.model = RandomForestClassifier(**default_params)
            
        elif self.model_type == 'logistic':
            default_params = {
                'C': 1.0,
                'max_iter': 1000,
                'random_state': self.random_state
            }
            default_params.update(params)
            self.model = LogisticRegression(**default_params)
            
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")
    
    def prepare_features(self, 
                        df: pd.DataFrame,
                        feature_cols: List[str],
                        target_col: str,
                        test_size: float = 0.2,
                        validate: bool = True) -> Dict:
        """
        å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        
        Args:
            df: è¾“å…¥æ•°æ®
            feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨
            target_col: ç›®æ ‡åˆ—å
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            validate: æ˜¯å¦è¿›è¡Œæ•°æ®éªŒè¯
            
        Returns:
            åŒ…å«è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å­—å…¸
        """
        logger.info("Preparing features and labels...")
        
        # éªŒè¯æ•°æ®
        if validate:
            self._validate_data(df, feature_cols, target_col)
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        X = df[feature_cols].copy()
        y = df[target_col].copy()
        
        # å¤„ç†ç¼ºå¤±å€¼
        X = X.fillna(X.mean())
        
        # å¤„ç†åˆ†ç±»å˜é‡
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns
        for col in categorical_cols:
            X[col] = pd.Categorical(X[col]).codes
        
        # ä¿å­˜ç‰¹å¾å
        self.feature_names = feature_cols
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state, stratify=y
        )
        
        # æ ‡å‡†åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # è½¬æ¢ä¸ºDataFrameä¿æŒç‰¹å¾å
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)
        
        logger.info(f"Training set size: {len(X_train_scaled)}, Test set size: {len(X_test_scaled)}")
        logger.info(f"Positive class ratio: {y.mean():.2%}")
        
        return {
            'X_train': X_train_scaled,
            'X_test': X_test_scaled,
            'y_train': y_train,
            'y_test': y_test
        }
    
    def _validate_data(self, df: pd.DataFrame, feature_cols: List[str], target_col: str):
        """éªŒè¯æ•°æ®è´¨é‡"""
        # æ£€æŸ¥ç¼ºå¤±åˆ—
        missing_cols = [col for col in feature_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing columns: {missing_cols}")
        
        if target_col not in df.columns:
            raise ValueError(f"Target column {target_col} not found")
        
        # æ£€æŸ¥ç›®æ ‡å˜é‡
        if df[target_col].nunique() < 2:
            raise ValueError("Target variable has only one unique value")
        
        # æ£€æŸ¥ç‰¹å¾æ–¹å·®
        for col in feature_cols:
            if df[col].nunique() == 1:
                logger.warning(f"Feature {col} has constant value, may not be useful")
    
    def train(self, 
             X_train: pd.DataFrame, 
             y_train: pd.Series,
             X_val: Optional[pd.DataFrame] = None,
             y_val: Optional[pd.Series] = None,
             **model_params) -> Dict:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            X_val: éªŒè¯ç‰¹å¾
            y_val: éªŒè¯æ ‡ç­¾
            model_params: æ¨¡å‹å‚æ•°
            
        Returns:
            è®­ç»ƒå†å²
        """
        logger.info(f"Training {self.model_type} model...")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model(**model_params)
        
        # è®­ç»ƒæ¨¡å‹
        if X_val is not None and y_val is not None:
            # æœ‰éªŒè¯é›†
            eval_set = [(X_train, y_train), (X_val, y_val)]
            self.model.fit(
                X_train, y_train,
                eval_set=eval_set,
                verbose=False
            )
        else:
            # æ— éªŒè¯é›†
            self.model.fit(X_train, y_train)
        
        # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
        train_pred = self.model.predict(X_train)
        train_proba = self.model.predict_proba(X_train)[:, 1]
        
        train_metrics = {
            'accuracy': accuracy_score(y_train, train_pred),
            'precision': precision_score(y_train, train_pred, zero_division=0),
            'recall': recall_score(y_train, train_pred, zero_division=0),
            'f1': f1_score(y_train, train_pred, zero_division=0),
            'auc': roc_auc_score(y_train, train_proba)
        }
        
        logger.info(f"Training metrics: {train_metrics}")
        
        return {'train_metrics': train_metrics}
    
    def evaluate(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•æ ‡ç­¾
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡
        """
        logger.info("Evaluating model...")
        
        # é¢„æµ‹
        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        self.metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1': f1_score(y_test, y_pred, zero_division=0),
            'auc': roc_auc_score(y_test, y_proba),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
            'classification_report': classification_report(y_test, y_pred, output_dict=True)
        }
        
        logger.info(f"Test metrics: {self.metrics}")
        
        return self.metrics
    
    def cross_validate(self, 
                       X: pd.DataFrame, 
                       y: pd.Series,
                       cv: int = 5,
                       **model_params) -> Dict:
        """
        äº¤å‰éªŒè¯
        
        Args:
            X: ç‰¹å¾
            y: æ ‡ç­¾
            cv: äº¤å‰éªŒè¯æŠ˜æ•°
            model_params: æ¨¡å‹å‚æ•°
            
        Returns:
            äº¤å‰éªŒè¯ç»“æœ
        """
        logger.info(f"Performing {cv}-fold cross validation...")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model(**model_params)
        
        # æ‰§è¡Œäº¤å‰éªŒè¯
        cv_scores = {
            'accuracy': cross_val_score(self.model, X, y, cv=cv, scoring='accuracy'),
            'precision': cross_val_score(self.model, X, y, cv=cv, scoring='precision'),
            'recall': cross_val_score(self.model, X, y, cv=cv, scoring='recall'),
            'f1': cross_val_score(self.model, X, y, cv=cv, scoring='f1'),
            'auc': cross_val_score(self.model, X, y, cv=cv, scoring='roc_auc')
        }
        
        cv_results = {}
        for metric, scores in cv_scores.items():
            cv_results[metric] = {
                'mean': scores.mean(),
                'std': scores.std(),
                'scores': scores.tolist()
            }
            logger.info(f"{metric}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
        
        return cv_results
    
    def hyperparameter_tuning(self,
                             X_train: pd.DataFrame,
                             y_train: pd.Series,
                             param_grid: Dict,
                             cv: int = 5,
                             scoring: str = 'roc_auc') -> Dict:
        """
        è¶…å‚æ•°è°ƒä¼˜
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            param_grid: å‚æ•°ç½‘æ ¼
            cv: äº¤å‰éªŒè¯æŠ˜æ•°
            scoring: è¯„åˆ†æŒ‡æ ‡
            
        Returns:
            æœ€ä½³å‚æ•°å’Œåˆ†æ•°
        """
        logger.info(f"Performing hyperparameter tuning with {cv}-fold CV...")
        
        # åˆå§‹åŒ–åŸºç¡€æ¨¡å‹
        self._init_model()
        
        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            self.model, param_grid, cv=cv, scoring=scoring,
            n_jobs=-1, verbose=1
        )
        grid_search.fit(X_train, y_train)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        self.model = grid_search.best_estimator_
        
        results = {
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'cv_results': grid_search.cv_results_
        }
        
        logger.info(f"Best params: {grid_search.best_params_}")
        logger.info(f"Best {scoring}: {grid_search.best_score_:.4f}")
        
        return results
    
    def get_feature_importance(self, feature_names: Optional[List[str]] = None) -> pd.DataFrame:
        """
        è·å–ç‰¹å¾é‡è¦æ€§
        
        Args:
            feature_names: ç‰¹å¾ååˆ—è¡¨
            
        Returns:
            ç‰¹å¾é‡è¦æ€§DataFrame
        """
        if feature_names is None:
            feature_names = self.feature_names
        
        if hasattr(self.model, 'feature_importances_'):
            # æ ‘æ¨¡å‹
            importance = self.model.feature_importances_
        elif hasattr(self.model, 'coef_'):
            # çº¿æ€§æ¨¡å‹
            importance = np.abs(self.model.coef_[0])
        else:
            logger.warning("Model does not provide feature importance")
            return pd.DataFrame()
        
        # åˆ›å»ºDataFrame
        self.feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        return self.feature_importance
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        é¢„æµ‹ç±»åˆ«
        
        Args:
            X: ç‰¹å¾
            
        Returns:
            é¢„æµ‹ç±»åˆ«
        """
        if self.model is None:
            raise ValueError("Model not trained yet")
        
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        é¢„æµ‹æ¦‚ç‡
        
        Args:
            X: ç‰¹å¾
            
        Returns:
            é¢„æµ‹æ¦‚ç‡
        """
        if self.model is None:
            raise ValueError("Model not trained yet")
        
        X_scaled = self.scaler.transform(X)
        return self.model.predict_proba(X_scaled)
    
    def save_model(self, filepath: str):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'model_type': self.model_type,
            'feature_names': self.feature_names,
            'metrics': self.metrics
        }
        joblib.dump(model_data, filepath)
        logger.info(f"Model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            filepath: æ¨¡å‹è·¯å¾„
        """
        model_data = joblib.load(filepath)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.model_type = model_data['model_type']
        self.feature_names = model_data['feature_names']
        self.metrics = model_data.get('metrics', {})
        logger.info(f"Model loaded from {filepath}")


class SatisfactionAnalyzer:
    """
    æ»¡æ„åº¦åˆ†æå™¨ï¼šç¾¤ä½“åˆ†æã€ç‰¹å¾åˆ†æã€é˜ˆå€¼ä¼˜åŒ–
    """
    
    def __init__(self, predictor: SatisfactionPredictor):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            predictor: è®­ç»ƒå¥½çš„é¢„æµ‹å™¨
        """
        self.predictor = predictor
        self.analysis_results = {}
    
    def analyze_by_segments(self,
                           df: pd.DataFrame,
                           segment_cols: List[str],
                           target_col: str,
                           pred_col: str = 'predicted_satisfaction') -> Dict:
        """
        æŒ‰ç¾¤ä½“åˆ†ææ»¡æ„åº¦
        
        Args:
            df: æ•°æ®
            segment_cols: åˆ†ç¾¤åˆ—å
            target_col: çœŸå®æ ‡ç­¾åˆ—
            pred_col: é¢„æµ‹æ ‡ç­¾åˆ—
            
        Returns:
            ç¾¤ä½“åˆ†æç»“æœ
        """
        logger.info("Analyzing satisfaction by segments...")
        
        segment_results = {}
        
        for col in segment_cols:
            if col not in df.columns:
                continue
            
            # è®¡ç®—å„ç¾¤ä½“çš„æŒ‡æ ‡
            segment_analysis = df.groupby(col).agg({
                target_col: ['mean', 'count'],
                pred_col: 'mean'
            }).round(4)
            
            segment_analysis.columns = ['actual_satisfaction', 'count', 'predicted_satisfaction']
            segment_analysis['error'] = abs(segment_analysis['actual_satisfaction'] - 
                                           segment_analysis['predicted_satisfaction'])
            
            segment_results[col] = segment_analysis.to_dict('index')
        
        self.analysis_results['segment_analysis'] = segment_results
        
        return segment_results
    
    def find_optimal_threshold(self,
                              y_true: pd.Series,
                              y_proba: np.ndarray,
                              metric: str = 'f1') -> Dict:
        """
        å¯»æ‰¾æœ€ä¼˜åˆ†ç±»é˜ˆå€¼
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_proba: é¢„æµ‹æ¦‚ç‡
            metric: ä¼˜åŒ–æŒ‡æ ‡ ('f1', 'precision', 'recall')
            
        Returns:
            æœ€ä¼˜é˜ˆå€¼å’Œå¯¹åº”çš„æŒ‡æ ‡
        """
        from sklearn.metrics import precision_recall_curve
        
        precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)
        
        if metric == 'f1':
            scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
        elif metric == 'precision':
            scores = precisions
        elif metric == 'recall':
            scores = recalls
        else:
            raise ValueError(f"Unsupported metric: {metric}")
        
        # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
        best_idx = np.argmax(scores[:-1])  # æ’é™¤æœ€åä¸€ä¸ª
        best_threshold = thresholds[best_idx]
        best_score = scores[best_idx]
        
        result = {
            'optimal_threshold': best_threshold,
            f'optimal_{metric}': best_score,
            'thresholds': thresholds.tolist(),
            'precisions': precisions.tolist(),
            'recalls': recalls.tolist()
        }
        
        self.analysis_results['threshold_optimization'] = result
        
        return result
    
    def analyze_errors(self,
                      df: pd.DataFrame,
                      feature_cols: List[str],
                      target_col: str,
                      pred_col: str = 'predicted') -> pd.DataFrame:
        """
        é”™è¯¯åˆ†æ
        
        Args:
            df: æ•°æ®
            feature_cols: ç‰¹å¾åˆ—
            target_col: çœŸå®æ ‡ç­¾
            pred_col: é¢„æµ‹æ ‡ç­¾
            
        Returns:
            é”™è¯¯æ ·æœ¬åˆ†æ
        """
        # æ ‡è®°é”™è¯¯ç±»å‹
        df_analysis = df.copy()
        df_analysis['error_type'] = 'correct'
        df_analysis.loc[(df_analysis[target_col] == 1) & (df_analysis[pred_col] == 0), 'error_type'] = 'false_negative'
        df_analysis.loc[(df_analysis[target_col] == 0) & (df_analysis[pred_col] == 1), 'error_type'] = 'false_positive'
        
        # åˆ†æå„ç‰¹å¾åœ¨é”™è¯¯æ ·æœ¬ä¸Šçš„åˆ†å¸ƒ
        error_analysis = {}
        
        for col in feature_cols:
            if col in df_analysis.columns:
                # è®¡ç®—å„é”™è¯¯ç±»å‹çš„ç‰¹å¾å‡å€¼
                error_stats = df_analysis.groupby('error_type')[col].agg(['mean', 'std', 'count'])
                error_analysis[col] = error_stats.to_dict()
        
        self.analysis_results['error_analysis'] = error_analysis
        
        return df_analysis
    
    def generate_insights(self) -> List[str]:
        """
        ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ
        
        Returns:
            æ´å¯Ÿåˆ—è¡¨
        """
        insights = []
        
        # ç‰¹å¾é‡è¦æ€§æ´å¯Ÿ
        if self.predictor.feature_importance is not None:
            top_features = self.predictor.feature_importance.head(3)
            insights.append(f"Top 3 important features: {', '.join(top_features['feature'].tolist())}")
        
        # æ¨¡å‹æ€§èƒ½æ´å¯Ÿ
        if self.predictor.metrics:
            metrics = self.predictor.metrics
            insights.append(f"Model AUC: {metrics.get('auc', 0):.3f}")
            insights.append(f"Model F1-score: {metrics.get('f1', 0):.3f}")
            
            if metrics.get('auc', 0) > 0.8:
                insights.append("Model shows strong predictive power")
            elif metrics.get('auc', 0) < 0.6:
                insights.append("Model needs improvement - consider adding more features")
        
        # é˜ˆå€¼ä¼˜åŒ–æ´å¯Ÿ
        if 'threshold_optimization' in self.analysis_results:
            thresh = self.analysis_results['threshold_optimization']
            insights.append(f"Optimal threshold: {thresh['optimal_threshold']:.3f} "
                          f"(F1: {thresh['optimal_f1']:.3f})")
        
        return insights


class SatisfactionVisualizer:
    """
    æ»¡æ„åº¦å¯è§†åŒ–å™¨
    """
    
    def __init__(self, predictor: SatisfactionPredictor):
        self.predictor = predictor
    
    def plot_feature_importance(self, top_n: int = 20, save_path: Optional[str] = None):
        """
        ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
        
        Args:
            top_n: æ˜¾ç¤ºå‰Nä¸ªç‰¹å¾
            save_path: ä¿å­˜è·¯å¾„
        """
        if self.predictor.feature_importance is None:
            logger.warning("No feature importance available")
            return
        
        importance_df = self.predictor.feature_importance.head(top_n)
        
        plt.figure(figsize=(10, 8))
        plt.barh(range(len(importance_df)), importance_df['importance'].values)
        plt.yticks(range(len(importance_df)), importance_df['feature'].values)
        plt.xlabel('Importance')
        plt.title(f'Top {top_n} Feature Importance')
        plt.gca().invert_yaxis()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_roc_curve(self, y_test: pd.Series, y_proba: np.ndarray, save_path: Optional[str] = None):
        """
        ç»˜åˆ¶ROCæ›²çº¿
        
        Args:
            y_test: çœŸå®æ ‡ç­¾
            y_proba: é¢„æµ‹æ¦‚ç‡
            save_path: ä¿å­˜è·¯å¾„
        """
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        auc = roc_auc_score(y_test, y_proba)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_confusion_matrix(self, y_test: pd.Series, y_pred: np.ndarray, save_path: Optional[str] = None):
        """
        ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        
        Args:
            y_test: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            save_path: ä¿å­˜è·¯å¾„
        """
        cm = confusion_matrix(y_test, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['Predicted Negative', 'Predicted Positive'],
                    yticklabels=['Actual Negative', 'Actual Positive'])
        plt.title('Confusion Matrix')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_prediction_distribution(self, y_test: pd.Series, y_proba: np.ndarray, save_path: Optional[str] = None):
        """
        ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            y_test: çœŸå®æ ‡ç­¾
            y_proba: é¢„æµ‹æ¦‚ç‡
            save_path: ä¿å­˜è·¯å¾„
        """
        plt.figure(figsize=(10, 6))
        
        # åˆ†åˆ«ç»˜åˆ¶æ­£è´Ÿæ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒ
        plt.hist(y_proba[y_test == 0], bins=30, alpha=0.5, label='Actual Negative', color='red')
        plt.hist(y_proba[y_test == 1], bins=30, alpha=0.5, label='Actual Positive', color='green')
        
        plt.xlabel('Predicted Probability')
        plt.ylabel('Frequency')
        plt.title('Prediction Probability Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_precision_recall_curve(self, y_test: pd.Series, y_proba: np.ndarray, save_path: Optional[str] = None):
        """
        ç»˜åˆ¶ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿
        
        Args:
            y_test: çœŸå®æ ‡ç­¾
            y_proba: é¢„æµ‹æ¦‚ç‡
            save_path: ä¿å­˜è·¯å¾„
        """
        from sklearn.metrics import precision_recall_curve
        
        precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)
        
        plt.figure(figsize=(10, 6))
        plt.plot(recalls, precisions, marker='.', label='PR Curve')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n = 10000
    
    # ç‰¹å¾
    df = pd.DataFrame({
        'user_id': range(n),
        'age': np.random.normal(35, 10, n),
        'investment_exp': np.random.exponential(5, n),
        'risk_score': np.random.uniform(0, 10, n),
        'query_frequency': np.random.poisson(10, n),
        'avg_rating_history': np.random.normal(3.5, 1, n),
        'nps_score_history': np.random.normal(0, 5, n),
        'query_length': np.random.normal(50, 20, n),
        'is_weekend': np.random.binomial(1, 0.3, n),
        'is_trading_hour': np.random.binomial(1, 0.6, n)
    })
    
    # ç”Ÿæˆæ»¡æ„åº¦æ ‡ç­¾ï¼ˆåŸºäºç‰¹å¾çš„é€»è¾‘ï¼‰
    logit = (0.1 * df['age'] + 
             0.2 * df['investment_exp'] + 
             -0.1 * df['risk_score'] + 
             0.15 * df['query_frequency'] + 
             0.3 * df['avg_rating_history'] + 
             -0.2 * (df['nps_score_history'] < -5) +
             np.random.normal(0, 1, n))
    
    prob = 1 / (1 + np.exp(-logit))
    df['satisfied'] = (prob > 0.5).astype(int)
    
    # ç‰¹å¾åˆ—
    feature_cols = ['age', 'investment_exp', 'risk_score', 'query_frequency',
                   'avg_rating_history', 'nps_score_history', 'query_length',
                   'is_weekend', 'is_trading_hour']
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = SatisfactionPredictor(model_type='xgboost')
    
    # å‡†å¤‡æ•°æ®
    data = predictor.prepare_features(
        df=df,
        feature_cols=feature_cols,
        target_col='satisfied',
        test_size=0.2
    )
    
    # è®­ç»ƒæ¨¡å‹
    predictor.train(data['X_train'], data['y_train'])
    
    # è¯„ä¼°æ¨¡å‹
    metrics = predictor.evaluate(data['X_test'], data['y_test'])
    print("\nğŸ“Š Model Performance:")
    for metric, value in metrics.items():
        if metric not in ['confusion_matrix', 'classification_report']:
            print(f"  {metric}: {value:.4f}")
    
    # ç‰¹å¾é‡è¦æ€§
    importance = predictor.get_feature_importance()
    print("\nğŸ” Top 5 Feature Importance:")
    print(importance.head())
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = SatisfactionAnalyzer(predictor)
    
    # æ·»åŠ é¢„æµ‹åˆ—
    df['predicted'] = predictor.predict(df[feature_cols])
    df['predicted_proba'] = predictor.predict_proba(df[feature_cols])[:, 1]
    
    # ç¾¤ä½“åˆ†æ
    segments = analyzer.analyze_by_segments(
        df=df,
        segment_cols=['is_weekend', 'is_trading_hour'],
        target_col='satisfied',
        pred_col='predicted'
    )
    print("\nğŸ‘¥ Segment Analysis:")
    print(segments)
    
    # é˜ˆå€¼ä¼˜åŒ–
    threshold = analyzer.find_optimal_threshold(
        data['y_test'],
        predictor.predict_proba(data['X_test'])[:, 1]
    )
    print(f"\nâš¡ Optimal threshold: {threshold['optimal_threshold']:.3f}")
    
    # ç”Ÿæˆæ´å¯Ÿ
    insights = analyzer.generate_insights()
    print("\nğŸ’¡ Business Insights:")
    for insight in insights:
        print(f"  â€¢ {insight}")
    
    # å¯è§†åŒ–
    visualizer = SatisfactionVisualizer(predictor)
    visualizer.plot_feature_importance(top_n=5)
    visualizer.plot_roc_curve(data['y_test'], predictor.predict_proba(data['X_test'])[:, 1])
    visualizer.plot_confusion_matrix(data['y_test'], predictor.predict(data['X_test']))
    
    # ä¿å­˜æ¨¡å‹
    predictor.save_model('models/satisfaction_model.pkl')
    
    # åŠ è½½æ¨¡å‹
    new_predictor = SatisfactionPredictor()
    new_predictor.load_model('models/satisfaction_model.pkl')